{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'train_testing_split' from 'sklearn.model_selection' (C:\\Users\\joela\\anaconda3\\envs\\STOR712\\lib\\site-packages\\sklearn\\model_selection\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[28], line 4\u001B[0m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m#Used to split into train and test set\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmodel_selection\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m train_testing_split\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m normalize\n\u001B[0;32m      8\u001B[0m \u001B[38;5;66;03m#Assumes that the dataset is in the current working directory under the name 'SkyData.csv'\u001B[39;00m\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'train_testing_split' from 'sklearn.model_selection' (C:\\Users\\joela\\anaconda3\\envs\\STOR712\\lib\\site-packages\\sklearn\\model_selection\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Used to split into train and test set\n",
    "from sklearn.model_selection import train_testing_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#Assumes that the dataset is in the current working directory under the name 'SkyData.csv'\n",
    "df = pd.read_csv('data/skyserver.csv')\n",
    "\n",
    "#Delete some irrelevant columns\n",
    "df.drop(['objid', 'rerun', 'specobjid', 'plate', 'mjd'], axis=1, inplace=True)\n",
    "\n",
    "#Remove the Quasar class from the dataframe (makes the problem binary classification)\n",
    "#Change class labels from string to 0 and 1\n",
    "df = df[df['class']!='QSO']\n",
    "df['class'] = pd.factorize(df['class'])[0]\n",
    "\n",
    "#Create a dataframe without the class label\n",
    "#Convert the dataframe to a numpy array\n",
    "#Add an extra column of ones to the data so the linear model has an offset\n",
    "Adf = df.drop('class', axis = 1)\n",
    "A = Adf.to_numpy()\n",
    "A = normalize(A)\n",
    "A = np.hstack((A, np.ones((A.shape[0], 1), dtype=A.dtype)))\n",
    "\n",
    "#Create a dataframe of only the class labels\n",
    "#Convert ydf to a numpy array\n",
    "ydf = df['class']\n",
    "y = ydf.to_numpy()\n",
    "\n",
    "#Split data into train and test set\n",
    "A_train,A_test,y_train,y_test = train_testing_split(A, y, testing_size=0.2)\n",
    "print(A_train.shape, A_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solutions\n",
    "\n",
    "Implement the following:\n",
    "1. Standard minibatch stochastic gradient\n",
    "2. Mini-batch stochastic gradient via random reshuffling\n",
    "3. SVRG\n",
    "\n",
    "Begin by defining the L2-regularized logistic regression model.\n",
    "Given data A and y and regularization parameter mu, compute either the function value or gradient for parameters x."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def logistic_regression_model(x, mode, A, y, mu):\n",
    "    \"\"\"\n",
    "    :param x: input value at which to evaluate function model\n",
    "    :param A: data feature matrix\n",
    "    :param y: data labels\n",
    "    :param mu: L2 regularization parameter\n",
    "    :return: logistic regression model value or gradient at parameters x\n",
    "    \"\"\"\n",
    "\n",
    "    # compute predicted values for parameters x given data A\n",
    "    predicted_values = np.matmul(A, x)\n",
    "\n",
    "    # compute either the function value or gradient of the logistic regression model\n",
    "    if mode == 'value':\n",
    "        output = (\n",
    "            np.mean(\n",
    "                -1 * y * predicted_values + np.log(1 + np.exp(predicted_values))\n",
    "            )\n",
    "            + mu / 2 * np.linalg.norm(x)\n",
    "        )\n",
    "    elif mode == 'gradient':\n",
    "        scalars = (-1 * y + (np.exp(predicted_values) / (1 + np.exp(predicted_values))))\n",
    "        output = np.matmul(A.T, scalars) / A.shape[0] + mu * x\n",
    "    else:\n",
    "        raise ValueError('mode must be either \"value\" or \"gradient\"')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# each epoch consists of a full batch gradient update\n",
    "def gradient_descent(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "\n",
    "        # compute gradient for all observations\n",
    "        gradient = fun(\n",
    "            x, 'gradient',  # compute\n",
    "            A_train, y_train  # data\n",
    "        )\n",
    "\n",
    "        # perform gradient step\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "        x -= alpha * gradient\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # store in output\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_losses)\n",
    "\n",
    "\n",
    "    return np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code chunks implement each of the new algorithmic implementations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# each epoch consists of the number of batches that fit the input data\n",
    "# each batch is randomly sampled without replacement\n",
    "def stochastic_gradient_uniformly_at_random(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, batch_size,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "    N = A_train.shape[0]\n",
    "    total_batches = N // batch_size\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "\n",
    "        # update learning rate\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "\n",
    "        for batch_number in range(total_batches):\n",
    "\n",
    "            # select random minibatch of size batch_size\n",
    "            batch_indices = np.random.permutation(N)[:batch_size]\n",
    "\n",
    "            # compute gradient for randomly selected observations\n",
    "            gradient = fun(\n",
    "                x, 'gradient',  # compute\n",
    "                A_train[batch_indices, :], y_train[batch_indices, :]  # data subset\n",
    "            )\n",
    "\n",
    "            # perform gradient step\n",
    "            x -= alpha * gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # store in output\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_losses)\n",
    "\n",
    "\n",
    "    return np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# each epoch consists of a random permutation of the data set\n",
    "# batches of size batch_size are processed at a time moving through the permutation\n",
    "# remaining observations smaller than the desired batch size will be discarded\n",
    "def stochastic_gradient_random_reshuffle(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, batch_size,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "    N = A_train.shape[0]\n",
    "    total_batches = N // batch_size\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "\n",
    "        # randomly permute the data at the beginning of each epoch\n",
    "        # note that data NOT divisible by the batch size will have data discarded\n",
    "        epoch_permutation = np.random.permutation(N)\n",
    "        A_train = A_train[epoch_permutation, :]\n",
    "        y_train = y_train[epoch_permutation, :]\n",
    "\n",
    "        # update learning rate\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "\n",
    "        for batch_number in range(total_batches):\n",
    "\n",
    "            # identify the chunk of the permutation to use for this batch\n",
    "            batch_indices = np.arange(\n",
    "                start = batch_number * batch_size,\n",
    "                stop = (batch_number + 1) * batch_size\n",
    "            )\n",
    "\n",
    "            # compute gradient for randomly selected observations\n",
    "            gradient = fun(\n",
    "                x, 'gradient',  # compute\n",
    "                A_train[batch_indices, :], y_train[batch_indices, :]  # data subset\n",
    "            )\n",
    "\n",
    "            # perform gradient step\n",
    "            x -= alpha * gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # store in output\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_losses)\n",
    "\n",
    "\n",
    "    return np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# source: Alg 5.1 of arxiv.org/pdf/1606.04838.pdf\n",
    "def stochastic_variance_reduced_gradient(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, m,  # additional hyper-parameters\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x_outer, x_inner = x0\n",
    "    alpha_bar = alpha\n",
    "\n",
    "    N = A_train.shape[0]\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_loss = fun(x_outer, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x_outer, 'value', A_test, y_test)\n",
    "\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "\n",
    "        # compute the full batch gradient\n",
    "        full_gradient = fun(\n",
    "            x_outer, 'gradient',  # compute\n",
    "            A_train, y_train  # data\n",
    "        )\n",
    "\n",
    "        # inner for loop iterations controlled by hyperparameter m\n",
    "        for j in range(m):\n",
    "\n",
    "            # identify the chunk of the permutation to use for this batch\n",
    "            observation_index = np.random.choice(N)\n",
    "\n",
    "            # compute gradient for randomly selected observation\n",
    "            # with respect to both inner and outer iterations\n",
    "            outer_gradient = fun(\n",
    "                x_outer, 'gradient',  # compute\n",
    "                A_train[observation_index, :], y_train[observation_index, :]  # data subset\n",
    "            )\n",
    "\n",
    "            inner_gradient = fun(\n",
    "                x_inner, 'gradient',  # compute\n",
    "                A_train[observation_index, :], y_train[observation_index, :]  # data subset\n",
    "            )\n",
    "\n",
    "            # compute gradient estimator\n",
    "            variance_reduced_gradient = inner_gradient - (outer_gradient - full_gradient)\n",
    "\n",
    "            # perform gradient step\n",
    "            x_inner -= alpha * variance_reduced_gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        x_outer = x_inner\n",
    "        training_loss = fun(x_outer, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x_outer, 'value', A_test, y_test)\n",
    "\n",
    "        # store in output\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_losses)\n",
    "\n",
    "\n",
    "    return np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the various descent methods for 100 total epochs with a batch size of 64 and a small regularization parameter of 10e-3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# defaults\n",
    "total_epochs = 100\n",
    "batch_size = 64\n",
    "mu = 10e-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tune-able hyper-pameters\n",
    "x0 = np.zeros(A_train.shape[-1])\n",
    "alpha = 0.01\n",
    "\n",
    "# standard gradient descent\n",
    "gradient_descent_training_losses, gradient_descent_testing_losses = gradient_descent(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs\n",
    ")\n",
    "\n",
    "methods = ['Fixed GD']\n",
    "methods_training_losses = [gradient_descent_training_losses]\n",
    "methods_testing_losses = [gradient_descent_testing_losses]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the various training and testing losses on seperate plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axarr = plt.subplots(ncols=2, sharey=True)\n",
    "\n",
    "# plot training losses\n",
    "plt.sca(axarr[0])\n",
    "for method_training_losses in methods_training_losses:\n",
    "    plt.plot(method_training_losses)\n",
    "\n",
    "# plot testing losses\n",
    "plt.sca(axarr[1])\n",
    "for method_testing_losses in methods_testing_losses:\n",
    "    plt.plot(method_testing_losses)\n",
    "\n",
    "plt.legend(methods)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc0521d73723d578bb29ab2609a41e433193533effbf9bc11ccfb972d3624215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
