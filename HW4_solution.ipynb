{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Homework 4"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import and clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71535, 13) (17884, 13) (71535,) (17884,)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#Used to split into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "#Assumes that the dataset is in the current working directory under the name 'SkyData.csv'\n",
    "df = pd.read_csv('data/skyserver.csv')\n",
    "\n",
    "#Delete some irrelevant columns\n",
    "df.drop(['objid', 'rerun', 'specobjid', 'plate', 'mjd'], axis=1, inplace=True)\n",
    "\n",
    "#Remove the Quasar class from the dataframe (makes the problem binary classification)\n",
    "#Change class labels from string to 0 and 1\n",
    "df = df[df['class']!='QSO']\n",
    "df['class'] = pd.factorize(df['class'])[0]\n",
    "\n",
    "#Create a dataframe without the class label\n",
    "#Convert the dataframe to a numpy array\n",
    "#Add an extra column of ones to the data so the linear model has an offset\n",
    "Adf = df.drop('class', axis = 1)\n",
    "A = Adf.to_numpy()\n",
    "A = normalize(A)\n",
    "A = np.hstack((A, np.ones((A.shape[0], 1), dtype=A.dtype)))\n",
    "\n",
    "#Create a dataframe of only the class labels\n",
    "#Convert ydf to a numpy array\n",
    "ydf = df['class']\n",
    "y = ydf.to_numpy()\n",
    "\n",
    "#Split data into train and test set\n",
    "A_train,A_test,y_train,y_test = train_test_split(A, y, test_size=0.2)\n",
    "print(A_train.shape, A_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Solutions\n",
    "\n",
    "Implement the following:\n",
    "1. Standard minibatch stochastic gradient\n",
    "2. Mini-batch stochastic gradient via random reshuffling\n",
    "3. SVRG\n",
    "\n",
    "Begin by defining the L2-regularized logistic regression model.\n",
    "Given data A and y and regularization parameter mu, compute either the function value or gradient for parameters x."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "outputs": [],
   "source": [
    "def logistic_regression_model(x, mode, A, y, mu):\n",
    "    \"\"\"\n",
    "    :param x: input value at which to evaluate function model\n",
    "    :param A: data feature matrix\n",
    "    :param y: data labels\n",
    "    :param mu: L2 regularization parameter\n",
    "    :return: logistic regression model value or gradient at parameters x\n",
    "    \"\"\"\n",
    "\n",
    "    # compute predicted values for parameters x given data A\n",
    "    predicted_values = np.matmul(A, x)\n",
    "\n",
    "    # compute either the function value or gradient of the logistic regression model\n",
    "    if mode == 'value':\n",
    "        output = (\n",
    "            np.mean(\n",
    "                -1 * y * predicted_values + np.log(1 + np.exp(predicted_values))\n",
    "            )\n",
    "            + mu / 2 * np.linalg.norm(x)\n",
    "        )\n",
    "    elif mode == 'gradient':\n",
    "        scalars = (-1 * y + (np.exp(predicted_values) / (1 + np.exp(predicted_values))))\n",
    "        output = np.matmul(A.T, scalars) / A.shape[0] + mu * x\n",
    "    else:\n",
    "        raise ValueError('mode must be either \"value\" or \"gradient\"')\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# each epoch consists of a full batch gradient update\n",
    "def gradient_descent(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "\n",
    "    gradient_computations = 0\n",
    "    gradient_computations_all = [gradient_computations]\n",
    "\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "    print(f'GD - Epoch 0, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # compute gradient for all observations\n",
    "        gradient = fun(\n",
    "            x, 'gradient',  # compute\n",
    "            A_train, y_train  # data\n",
    "        )\n",
    "        gradient_computations = gradient_computations + 1\n",
    "\n",
    "        # perform gradient step\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "        x = x - alpha * gradient\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # print epoch summary\n",
    "        epoch_end = time.time()\n",
    "        if (epoch_number + 1) % 10 == 0:\n",
    "            print(f'GD - Epoch {epoch_number + 1:d}, Time {epoch_end - epoch_start:.1f}, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "        # store in output\n",
    "        gradient_computations_all.append(gradient_computations)\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_loss)\n",
    "\n",
    "\n",
    "    return np.array(gradient_computations_all), np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code chunks implement each of the new algorithmic implementations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "outputs": [],
   "source": [
    "# each epoch consists of the number of batches that fit the input data\n",
    "# each batch is randomly sampled without replacement\n",
    "def stochastic_gradient_uniformly_at_random(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, batch_size,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "\n",
    "    gradient_computations = 0\n",
    "    gradient_computations_all = [gradient_computations]\n",
    "\n",
    "    N = A_train.shape[0]\n",
    "    total_batches = N // batch_size\n",
    "    print(f'Observations: {N}, Batch Size: {batch_size}, Total Batches: {total_batches}')\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "    print(f'SGM Uniform - Epoch 0, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # update learning rate\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "\n",
    "        for batch_number in range(total_batches):\n",
    "\n",
    "            # select random minibatch of size batch_size\n",
    "            batch_indices = np.random.randint(\n",
    "                low=0,\n",
    "                high=N,\n",
    "                size=batch_size\n",
    "            )\n",
    "\n",
    "            # compute gradient for randomly selected observations\n",
    "            gradient = fun(\n",
    "                x, 'gradient',  # compute\n",
    "                A_train[batch_indices, :], y_train[batch_indices]  # data subset\n",
    "            )\n",
    "            gradient_computations = gradient_computations + 1\n",
    "\n",
    "            # perform gradient step\n",
    "            x = x - alpha * gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # print epoch summary\n",
    "        epoch_end = time.time()\n",
    "        if (epoch_number + 1) % 10 == 0:\n",
    "            print(f'SGM Uniform - Epoch {epoch_number + 1:d}, Time {epoch_end - epoch_start:.1f}, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "        # store in output\n",
    "        gradient_computations_all.append(gradient_computations)\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_loss)\n",
    "\n",
    "\n",
    "    return np.array(gradient_computations_all), np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "outputs": [],
   "source": [
    "# each epoch consists of a random permutation of the data set\n",
    "# batches of size batch_size are processed at a time moving through the permutation\n",
    "# remaining observations smaller than the desired batch size will be discarded\n",
    "def stochastic_gradient_random_reshuffle(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, batch_size,\n",
    "    decay=False  # learning rate decay of form alpha_k = alpha / k\n",
    "):\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x = x0\n",
    "    alpha_bar = alpha\n",
    "\n",
    "    gradient_computations = 0\n",
    "    gradient_computations_all = [gradient_computations]\n",
    "\n",
    "    N = A_train.shape[0]\n",
    "    total_batches = N // batch_size\n",
    "    print(f'Observations: {N}, Batch Size: {batch_size}, Total Batches: {total_batches}')\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_loss = fun(x, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x, 'value', A_test, y_test)\n",
    "    print(f'SGM Shuffle - Epoch 0, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # randomly permute the data at the beginning of each epoch\n",
    "        # note that data NOT divisible by the batch size will have data discarded\n",
    "        epoch_permutation = np.random.permutation(N)\n",
    "        A_train = A_train[epoch_permutation, :]\n",
    "        y_train = y_train[epoch_permutation]\n",
    "\n",
    "        # update learning rate\n",
    "        if decay:\n",
    "            alpha = alpha_bar / (epoch_number + 1)\n",
    "\n",
    "        for batch_number in range(total_batches):\n",
    "\n",
    "            # identify the chunk of the permutation to use for this batch\n",
    "            batch_indices = np.arange(\n",
    "                start = batch_number * batch_size,\n",
    "                stop = (batch_number + 1) * batch_size\n",
    "            )\n",
    "\n",
    "            # compute gradient for randomly selected observations\n",
    "            gradient = fun(\n",
    "                x, 'gradient',  # compute\n",
    "                A_train[batch_indices, :], y_train[batch_indices]  # data subset\n",
    "            )\n",
    "            gradient_computations = gradient_computations + 1\n",
    "\n",
    "            # perform gradient step\n",
    "            x = x - alpha * gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        training_loss = fun(x, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x, 'value', A_test, y_test)\n",
    "\n",
    "        # print epoch summary\n",
    "        epoch_end = time.time()\n",
    "        if (epoch_number + 1) % 10 == 0:\n",
    "            print(f'SGM Shuffle - Epoch {epoch_number + 1:d}, Time {epoch_end - epoch_start:.1f}, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "        # store in output\n",
    "        gradient_computations_all.append(gradient_computations)\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_loss)\n",
    "\n",
    "\n",
    "    return np.array(gradient_computations_all), np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [],
   "source": [
    "# source: Alg 5.1 of arxiv.org/pdf/1606.04838.pdf\n",
    "def stochastic_variance_reduced_gradient(\n",
    "    fun,  # objective function\n",
    "    A_train, A_test, y_train, y_test,  # data\n",
    "    x0, alpha,  # algorithmic initializations\n",
    "    total_epochs, batch_size  # additional hyperparameters\n",
    "):\n",
    "    \"\"\"\n",
    "    :param batch_size: used to compute hyperparameter m, will have double the gradient calculation\n",
    "    \"\"\"\n",
    "\n",
    "    # initialize algorithm at initial point\n",
    "    x_outer, x_inner = x0, x0\n",
    "    alpha_bar = alpha\n",
    "\n",
    "    gradient_computations = 0\n",
    "    gradient_computations_all = []\n",
    "\n",
    "    # the number of total batches\n",
    "    N = A_train.shape[0]\n",
    "    total_batches = N // batch_size\n",
    "    print(f'Observations: {N}, Batch Size: {batch_size}, Total Batches: {total_batches}')\n",
    "\n",
    "    # initialize objects to store output\n",
    "    training_loss = fun(x_outer, 'value', A_train, y_train)\n",
    "    testing_loss = fun(x_outer, 'value', A_test, y_test)\n",
    "    print(f'SVRG - Epoch 0, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "    training_losses = [training_loss]\n",
    "    testing_losses = [testing_loss]\n",
    "\n",
    "    # run until maximum iterations are reached\n",
    "    for epoch_number in range(total_epochs):\n",
    "        epoch_start = time.time()\n",
    "\n",
    "        # compute the full batch gradient\n",
    "        full_gradient = fun(\n",
    "            x_outer, 'gradient',  # compute\n",
    "            A_train, y_train  # data\n",
    "        )\n",
    "        gradient_computations = gradient_computations + 1\n",
    "\n",
    "        # inner for loop iterations controlled by hyperparameter m\n",
    "        for j in range(total_batches):\n",
    "\n",
    "            # compute gradient for randomly selected observation\n",
    "            observation_index = np.random.choice(N)\n",
    "\n",
    "            # with respect to both inner and outer iterations\n",
    "            outer_gradient = fun(\n",
    "                x_outer, 'gradient',  # compute\n",
    "                A_train[[observation_index], :], y_train[observation_index]  # data subset\n",
    "            )\n",
    "            gradient_computations = gradient_computations + 1\n",
    "\n",
    "            inner_gradient = fun(\n",
    "                x_inner, 'gradient',  # compute\n",
    "                A_train[[observation_index], :], y_train[observation_index]  # data subset\n",
    "            )\n",
    "            gradient_computations = gradient_computations + 1\n",
    "\n",
    "            # compute gradient estimator\n",
    "            variance_reduced_gradient = inner_gradient - (outer_gradient - full_gradient)\n",
    "\n",
    "            # perform gradient step\n",
    "            x_inner = x_inner - alpha * variance_reduced_gradient\n",
    "\n",
    "\n",
    "        # update training and test losses at batch end\n",
    "        x_outer = x_inner\n",
    "        training_loss = fun(x_outer, 'value', A_train, y_train)\n",
    "        testing_loss = fun(x_outer, 'value', A_test, y_test)\n",
    "\n",
    "        epoch_end = time.time()\n",
    "        if (epoch_number + 1) % 10 == 0:\n",
    "            print(f'SVRG - Epoch {epoch_number + 1:d}, Time {epoch_end - epoch_start:.1f}, Training Loss: {training_loss:.4f}')\n",
    "\n",
    "        # store in output\n",
    "        gradient_computations_all.append(gradient_computations)\n",
    "        training_losses.append(training_loss)\n",
    "        testing_losses.append(testing_loss)\n",
    "\n",
    "\n",
    "    return np.array(gradient_computations_all), np.array(training_losses), np.array(testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Perform the various descent methods for 100 total epochs with a batch size of 64 and a small regularization parameter of 10e-3."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "outputs": [],
   "source": [
    "# defaults\n",
    "total_epochs = 100\n",
    "batch_size = 64\n",
    "mu = 10e-3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GD - Epoch 0, Training Loss: 127.0848\n",
      "GD - Epoch 10, Time 0.0, Training Loss: 61.6467\n",
      "GD - Epoch 20, Time 0.0, Training Loss: 22.4815\n",
      "GD - Epoch 30, Time 0.0, Training Loss: 1.5889\n",
      "GD - Epoch 40, Time 0.0, Training Loss: 1.1385\n",
      "GD - Epoch 50, Time 0.0, Training Loss: 0.9044\n",
      "GD - Epoch 60, Time 0.0, Training Loss: 0.8190\n",
      "GD - Epoch 70, Time 0.0, Training Loss: 0.9708\n",
      "GD - Epoch 80, Time 0.0, Training Loss: 0.9534\n",
      "GD - Epoch 90, Time 0.0, Training Loss: 0.9438\n",
      "GD - Epoch 100, Time 0.0, Training Loss: 0.9386\n"
     ]
    }
   ],
   "source": [
    "# tune-able hyper-parameters\n",
    "x0 = 100 * np.random.normal(size = A_train.shape[-1])\n",
    "alpha = 5\n",
    "\n",
    "# standard gradient descent\n",
    "_, gradient_descent_training_losses, gradient_descent_testing_losses = gradient_descent(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs\n",
    ")\n",
    "\n",
    "methods = ['Fixed GD']\n",
    "methods_training_losses = [gradient_descent_training_losses]\n",
    "methods_testing_losses = [gradient_descent_testing_losses]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 71535, Batch Size: 64, Total Batches: 1117\n",
      "SGM Uniform - Epoch 0, Training Loss: 0.6931\n",
      "SGM Uniform - Epoch 10, Time 2.7, Training Loss: 0.6829\n",
      "SGM Uniform - Epoch 20, Time 2.0, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 30, Time 2.4, Training Loss: 0.6830\n",
      "SGM Uniform - Epoch 40, Time 2.1, Training Loss: 0.6830\n",
      "SGM Uniform - Epoch 50, Time 1.9, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 60, Time 2.5, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 70, Time 2.0, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 80, Time 1.8, Training Loss: 0.6829\n",
      "SGM Uniform - Epoch 90, Time 1.7, Training Loss: 0.6829\n",
      "SGM Uniform - Epoch 100, Time 1.9, Training Loss: 0.6831\n"
     ]
    }
   ],
   "source": [
    "# stochastic gradient descents\n",
    "_, uniformly_at_random_training_losses, uniformly_at_random_testing_losses = stochastic_gradient_uniformly_at_random(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs, batch_size\n",
    ")\n",
    "\n",
    "methods.append('Fixed SGM Uniform')\n",
    "methods_training_losses.append(uniformly_at_random_training_losses)\n",
    "methods_testing_losses.append(uniformly_at_random_testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 71535, Batch Size: 64, Total Batches: 1117\n",
      "SGM Uniform - Epoch 0, Training Loss: 0.6931\n",
      "SGM Uniform - Epoch 10, Time 1.7, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 20, Time 1.7, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 30, Time 1.7, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 40, Time 2.3, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 50, Time 1.9, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 60, Time 1.8, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 70, Time 1.9, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 80, Time 2.0, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 90, Time 2.0, Training Loss: 0.6828\n",
      "SGM Uniform - Epoch 100, Time 1.9, Training Loss: 0.6828\n"
     ]
    }
   ],
   "source": [
    "_, uniformly_at_random_decay_training_losses, uniformly_at_random_decay_testing_losses = stochastic_gradient_uniformly_at_random(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs, batch_size,\n",
    "    decay=True\n",
    ")\n",
    "\n",
    "methods.append('Decay SGM Uniform')\n",
    "methods_training_losses.append(uniformly_at_random_decay_training_losses)\n",
    "methods_testing_losses.append(uniformly_at_random_decay_testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 71535, Batch Size: 64, Total Batches: 1117\n",
      "SGM Shuffle - Epoch 0, Training Loss: 0.9124\n",
      "SGM Shuffle - Epoch 10, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 20, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 30, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 40, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 50, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 60, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 70, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 80, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 90, Time 0.1, Training Loss: 0.6834\n",
      "SGM Shuffle - Epoch 100, Time 0.1, Training Loss: 0.6830\n"
     ]
    }
   ],
   "source": [
    "_, random_reshuffle_training_losses, random_reshuffle_testing_losses = stochastic_gradient_random_reshuffle(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs, batch_size\n",
    ")\n",
    "\n",
    "methods.append('Fixed SGM Shuffle')\n",
    "methods_training_losses.append(random_reshuffle_training_losses)\n",
    "methods_testing_losses.append(random_reshuffle_testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 71535, Batch Size: 64, Total Batches: 1117\n",
      "SGM Shuffle - Epoch 0, Training Loss: 127.0848\n",
      "SGM Shuffle - Epoch 10, Time 0.0, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 20, Time 0.2, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 30, Time 0.0, Training Loss: 0.6837\n",
      "SGM Shuffle - Epoch 40, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 50, Time 0.1, Training Loss: 0.6831\n",
      "SGM Shuffle - Epoch 60, Time 0.0, Training Loss: 0.6831\n",
      "SGM Shuffle - Epoch 70, Time 0.0, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 80, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 90, Time 0.1, Training Loss: 0.6830\n",
      "SGM Shuffle - Epoch 100, Time 0.0, Training Loss: 0.6831\n"
     ]
    }
   ],
   "source": [
    "_, random_reshuffle_decay_training_losses, random_reshuffle_decay_testing_losses = stochastic_gradient_random_reshuffle(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    total_epochs, batch_size,\n",
    "    decay=True\n",
    ")\n",
    "\n",
    "methods.append('Decay SGM Shuffle')\n",
    "methods_training_losses.append(random_reshuffle_decay_training_losses)\n",
    "methods_testing_losses.append(random_reshuffle_decay_testing_losses)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations: 71535, Batch Size: 1, Total Batches: 71535\n",
      "SVRG - Epoch 0, Training Loss: 0.6931\n",
      "SVRG - Epoch 10, Time 4.4, Training Loss: 0.6829\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[152], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# variance reduced gradient estimation algorithm\u001B[39;00m\n\u001B[1;32m----> 4\u001B[0m _, variance_reduced_training_losses, variance_reduced_testing_losses \u001B[38;5;241m=\u001B[39m \u001B[43mstochastic_variance_reduced_gradient\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogistic_regression_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mb\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmu\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mA_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mA_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmath\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mceil\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtotal_epochs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m/\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      9\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\n\u001B[0;32m     10\u001B[0m \u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[125], line 50\u001B[0m, in \u001B[0;36mstochastic_variance_reduced_gradient\u001B[1;34m(fun, A_train, A_test, y_train, y_test, x0, alpha, total_epochs, batch_size)\u001B[0m\n\u001B[0;32m     47\u001B[0m observation_index \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mchoice(N)\n\u001B[0;32m     49\u001B[0m \u001B[38;5;66;03m# with respect to both inner and outer iterations\u001B[39;00m\n\u001B[1;32m---> 50\u001B[0m outer_gradient \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     51\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx_outer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mgradient\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# compute\u001B[39;49;00m\n\u001B[0;32m     52\u001B[0m \u001B[43m    \u001B[49m\u001B[43mA_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43mobservation_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m[\u001B[49m\u001B[43mobservation_index\u001B[49m\u001B[43m]\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# data subset\u001B[39;49;00m\n\u001B[0;32m     53\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     54\u001B[0m gradient_computations \u001B[38;5;241m=\u001B[39m gradient_computations \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     56\u001B[0m inner_gradient \u001B[38;5;241m=\u001B[39m fun(\n\u001B[0;32m     57\u001B[0m     x_inner, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgradient\u001B[39m\u001B[38;5;124m'\u001B[39m,  \u001B[38;5;66;03m# compute\u001B[39;00m\n\u001B[0;32m     58\u001B[0m     A_train[[observation_index], :], y_train[observation_index]  \u001B[38;5;66;03m# data subset\u001B[39;00m\n\u001B[0;32m     59\u001B[0m )\n",
      "Cell \u001B[1;32mIn[152], line 5\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(x, y, A, b)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# variance reduced gradient estimation algorithm\u001B[39;00m\n\u001B[0;32m      4\u001B[0m _, variance_reduced_training_losses, variance_reduced_testing_losses \u001B[38;5;241m=\u001B[39m stochastic_variance_reduced_gradient(\n\u001B[1;32m----> 5\u001B[0m     \u001B[38;5;28;01mlambda\u001B[39;00m x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n\u001B[0;32m      6\u001B[0m     A_train, A_test, y_train, y_test,\n\u001B[0;32m      7\u001B[0m     x0, alpha,\n\u001B[0;32m      8\u001B[0m     math\u001B[38;5;241m.\u001B[39mceil(total_epochs \u001B[38;5;241m/\u001B[39m \u001B[38;5;241m2\u001B[39m),\n\u001B[0;32m      9\u001B[0m     batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m\n\u001B[0;32m     10\u001B[0m )\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# variance reduced gradient estimation algorithm\n",
    "_, variance_reduced_training_losses, variance_reduced_testing_losses = stochastic_variance_reduced_gradient(\n",
    "    lambda x, y, A, b: logistic_regression_model(x, y, A, b, mu),\n",
    "    A_train, A_test, y_train, y_test,\n",
    "    x0, alpha,\n",
    "    math.ceil(total_epochs / 2),\n",
    "    batch_size=1\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Plot the various training and testing losses on seperate plots"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 800x400 with 2 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axarr = plt.subplots(ncols=2, sharey=True, figsize=(8, 4))\n",
    "\n",
    "# plot training losses\n",
    "plt.sca(axarr[0])\n",
    "for method_training_losses in methods_training_losses:\n",
    "    plt.plot(np.arange(len(method_training_losses)), method_training_losses)\n",
    "#plt.plot(2 * np.arange(len(variance_reduced_training_losses)), variance_reduced_training_losses)\n",
    "\n",
    "# plot testing losses\n",
    "plt.sca(axarr[1])\n",
    "for method_testing_losses in methods_testing_losses:\n",
    "    plt.semilogy(np.arange(len(method_testing_losses)), method_testing_losses)\n",
    "#plt.semilogy(2 * np.arange(len(variance_reduced_testing_losses)), variance_reduced_testing_losses)\n",
    "\n",
    "plt.legend(methods)\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    }
   ],
   "source": [
    "print(len(methods), len(methods_training_losses))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fc0521d73723d578bb29ab2609a41e433193533effbf9bc11ccfb972d3624215"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
